{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from util import load_data, separate_data\n",
    "from models.graphcnn import GraphCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'dataset': 'MUTAG',\n",
    "    'device': 0,\n",
    "    'batch_size': 32,\n",
    "    'iters_per_epoch': 50,\n",
    "    'epochs': 5,\n",
    "    'lr': 0.01,\n",
    "    'seed': 0,\n",
    "    'fold_idx': 0,\n",
    "    'num_layers': 5,\n",
    "    'num_mlp_layers': 2,\n",
    "    'hidden_dim': 64,\n",
    "    'final_dropout': 0.5,\n",
    "    'graph_pooling_type': 'sum',\n",
    "    'neighbor_pooling_type': 'sum',\n",
    "    'learn_eps': False,\n",
    "    'degree_as_tag': False,\n",
    "    'filename': \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_graphs, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_iters = args['iters_per_epoch']\n",
    "    pbar = tqdm(range(total_iters), unit='batch')\n",
    "\n",
    "    loss_accum = 0\n",
    "    for pos in pbar:\n",
    "        selected_idx = np.random.permutation(len(train_graphs))[:args['batch_size']]\n",
    "\n",
    "        batch_graph = [train_graphs[idx] for idx in selected_idx]\n",
    "        output = model(batch_graph)\n",
    "\n",
    "        labels = torch.LongTensor([graph.label for graph in batch_graph]).to(device)\n",
    "\n",
    "        #compute loss\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        #backprop\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()         \n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "        loss = loss.detach().cpu().numpy()\n",
    "        loss_accum += loss\n",
    "\n",
    "        #report\n",
    "        pbar.set_description('epoch: %d' % (epoch))\n",
    "\n",
    "    average_loss = loss_accum/total_iters\n",
    "    print(\"loss training: %f\" % (average_loss))\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "###pass data to model with minibatch during testing to avoid memory overflow (does not perform backpropagation)\n",
    "def pass_data_iteratively(model, graphs, minibatch_size = 64):\n",
    "    model.eval()\n",
    "    output = []\n",
    "    idx = np.arange(len(graphs))\n",
    "    for i in range(0, len(graphs), minibatch_size):\n",
    "        sampled_idx = idx[i:i+minibatch_size]\n",
    "        if len(sampled_idx) == 0:\n",
    "            continue\n",
    "        output.append(model([graphs[j] for j in sampled_idx]).detach())\n",
    "    return torch.cat(output, 0)\n",
    "\n",
    "def test(args, model, device, train_graphs, test_graphs, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    output = pass_data_iteratively(model, train_graphs)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    labels = torch.LongTensor([graph.label for graph in train_graphs]).to(device)\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().cpu().item()\n",
    "    acc_train = correct / float(len(train_graphs))\n",
    "\n",
    "    output = pass_data_iteratively(model, test_graphs)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    labels = torch.LongTensor([graph.label for graph in test_graphs]).to(device)\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().cpu().item()\n",
    "    acc_test = correct / float(len(test_graphs))\n",
    "\n",
    "    print(\"accuracy train: %f test: %f\" % (acc_train, acc_test))\n",
    "\n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatsGraphCNN(GraphCNN):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def __preprocess_neighbors_maxpool(self, batch_graph):\n",
    "        ###create padded_neighbor_list in concatenated graph\n",
    "\n",
    "        #compute the maximum number of neighbors within the graphs in the current minibatch\n",
    "        max_deg = max([graph.max_neighbor for graph in batch_graph])\n",
    "\n",
    "        padded_neighbor_list = []\n",
    "        start_idx = [0]\n",
    "\n",
    "\n",
    "        for i, graph in enumerate(batch_graph):\n",
    "            start_idx.append(start_idx[i] + len(graph.g))\n",
    "            padded_neighbors = []\n",
    "            for j in range(len(graph.neighbors)):\n",
    "                #add off-set values to the neighbor indices\n",
    "                pad = [n + start_idx[i] for n in graph.neighbors[j]]\n",
    "                #padding, dummy data is assumed to be stored in -1\n",
    "                pad.extend([-1]*(max_deg - len(pad)))\n",
    "\n",
    "                #Add center nodes in the maxpooling if learn_eps is False, i.e., aggregate center nodes and neighbor nodes altogether.\n",
    "                if not self.learn_eps:\n",
    "                    pad.append(j + start_idx[i])\n",
    "\n",
    "                padded_neighbors.append(pad)\n",
    "            padded_neighbor_list.extend(padded_neighbors)\n",
    "\n",
    "        return torch.LongTensor(padded_neighbor_list)\n",
    "\n",
    "\n",
    "    def __preprocess_neighbors_sumavepool(self, batch_graph):\n",
    "        ###create block diagonal sparse matrix\n",
    "\n",
    "        edge_mat_list = []\n",
    "        start_idx = [0]\n",
    "        for i, graph in enumerate(batch_graph):\n",
    "            start_idx.append(start_idx[i] + len(graph.g))\n",
    "            edge_mat_list.append(graph.edge_mat + start_idx[i])\n",
    "        Adj_block_idx = torch.cat(edge_mat_list, 1)\n",
    "        Adj_block_elem = torch.ones(Adj_block_idx.shape[1])\n",
    "\n",
    "        #Add self-loops in the adjacency matrix if learn_eps is False, i.e., aggregate center nodes and neighbor nodes altogether.\n",
    "\n",
    "        if not self.learn_eps:\n",
    "            num_node = start_idx[-1]\n",
    "            self_loop_edge = torch.LongTensor([range(num_node), range(num_node)])\n",
    "            elem = torch.ones(num_node)\n",
    "            Adj_block_idx = torch.cat([Adj_block_idx, self_loop_edge], 1)\n",
    "            Adj_block_elem = torch.cat([Adj_block_elem, elem], 0)\n",
    "\n",
    "        Adj_block = torch.sparse.FloatTensor(Adj_block_idx, Adj_block_elem, torch.Size([start_idx[-1],start_idx[-1]]))\n",
    "\n",
    "        return Adj_block.to(self.device)\n",
    "\n",
    "\n",
    "    def __preprocess_graphpool(self, batch_graph):\n",
    "        ###create sum or average pooling sparse matrix over entire nodes in each graph (num graphs x num nodes)\n",
    "        \n",
    "        start_idx = [0]\n",
    "\n",
    "        #compute the padded neighbor list\n",
    "        for i, graph in enumerate(batch_graph):\n",
    "            start_idx.append(start_idx[i] + len(graph.g))\n",
    "\n",
    "        idx = []\n",
    "        elem = []\n",
    "        for i, graph in enumerate(batch_graph):\n",
    "            ###average pooling\n",
    "            if self.graph_pooling_type == \"average\":\n",
    "                elem.extend([1./len(graph.g)]*len(graph.g))\n",
    "            \n",
    "            else:\n",
    "            ###sum pooling\n",
    "                elem.extend([1]*len(graph.g))\n",
    "\n",
    "            idx.extend([[i, j] for j in range(start_idx[i], start_idx[i+1], 1)])\n",
    "        elem = torch.FloatTensor(elem)\n",
    "        idx = torch.LongTensor(idx).transpose(0,1)\n",
    "        graph_pool = torch.sparse.FloatTensor(idx, elem, torch.Size([len(batch_graph), start_idx[-1]]))\n",
    "        \n",
    "        return graph_pool.to(self.device)\n",
    "    \n",
    "    def features(self, batch_graph):\n",
    "        X_concat = torch.cat([graph.node_features for graph in batch_graph], 0).to(self.device)\n",
    "        graph_pool = self.__preprocess_graphpool(batch_graph)\n",
    "\n",
    "        if self.neighbor_pooling_type == \"max\":\n",
    "            padded_neighbor_list = self.__preprocess_neighbors_maxpool(batch_graph)\n",
    "        else:\n",
    "            Adj_block = self.__preprocess_neighbors_sumavepool(batch_graph)\n",
    "\n",
    "        #list of hidden representation at each layer (including input)\n",
    "        hidden_rep = [X_concat]\n",
    "        h = X_concat\n",
    "\n",
    "        for layer in range(self.num_layers-1):\n",
    "            if self.neighbor_pooling_type == \"max\" and self.learn_eps:\n",
    "                h = self.next_layer_eps(h, layer, padded_neighbor_list = padded_neighbor_list)\n",
    "            elif not self.neighbor_pooling_type == \"max\" and self.learn_eps:\n",
    "                h = self.next_layer_eps(h, layer, Adj_block = Adj_block)\n",
    "            elif self.neighbor_pooling_type == \"max\" and not self.learn_eps:\n",
    "                h = self.next_layer(h, layer, padded_neighbor_list = padded_neighbor_list)\n",
    "            elif not self.neighbor_pooling_type == \"max\" and not self.learn_eps:\n",
    "                h = self.next_layer(h, layer, Adj_block = Adj_block)\n",
    "\n",
    "            hidden_rep.append(h)\n",
    "        \n",
    "        pooleds = [graph_pool]\n",
    "        #perform pooling over all nodes in each graph in every layer\n",
    "        for layer, h in enumerate(hidden_rep):\n",
    "            pooled_h = torch.spmm(graph_pool, h)\n",
    "            pooleds.append(pooled_h)\n",
    "            \n",
    "        return hidden_rep, pooleds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "# classes: 2\n",
      "# maximum node tag: 7\n",
      "# data: 188\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#set up seeds and gpu device\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)    \n",
    "device = torch.device(\"cuda:\" + str(args['device'])) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "graphs, num_classes = load_data(args['dataset'], args['degree_as_tag'])\n",
    "\n",
    "##10-fold cross validation. Conduct an experiment on the fold specified by args.fold_idx.\n",
    "train_graphs, test_graphs = separate_data(graphs, args['seed'], args['fold_idx'])\n",
    "\n",
    "model = FeatsGraphCNN(args['num_layers'], args['num_mlp_layers'], train_graphs[0].node_features.shape[1], args['hidden_dim'], num_classes, args['final_dropout'], args['learn_eps'], args['graph_pooling_type'], args['neighbor_pooling_type'], device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgar/anaconda3/envs/geo-const/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "epoch: 1: 100%|██████████| 50/50 [00:01<00:00, 25.04batch/s]\n",
      "epoch: 2:   6%|▌         | 3/50 [00:00<00:01, 26.27batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 2.456600\n",
      "accuracy train: 0.702381 test: 0.650000\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2: 100%|██████████| 50/50 [00:01<00:00, 26.10batch/s]\n",
      "epoch: 3:   6%|▌         | 3/50 [00:00<00:01, 25.06batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.906911\n",
      "accuracy train: 0.761905 test: 0.750000\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3: 100%|██████████| 50/50 [00:01<00:00, 25.13batch/s]\n",
      "epoch: 4:   6%|▌         | 3/50 [00:00<00:01, 24.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.738349\n",
      "accuracy train: 0.821429 test: 0.800000\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4: 100%|██████████| 50/50 [00:01<00:00, 25.35batch/s]\n",
      "epoch: 5:   6%|▌         | 3/50 [00:00<00:01, 26.25batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.969491\n",
      "accuracy train: 0.559524 test: 0.500000\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5: 100%|██████████| 50/50 [00:01<00:00, 25.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.573871\n",
      "accuracy train: 0.779762 test: 0.700000\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_loss = train(args, model, device, train_graphs, optimizer, epoch)\n",
    "    acc_train, acc_test = test(args, model, device, train_graphs, test_graphs, epoch)\n",
    "\n",
    "    if not args['filename'] == \"\":\n",
    "        with open(args['filename'], 'w') as f:\n",
    "            f.write(\"%f %f %f\" % (avg_loss, acc_train, acc_test))\n",
    "            f.write(\"\\n\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(model.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = np.random.permutation(len(train_graphs))[:args['batch_size']]\n",
    "\n",
    "batch_graph = [train_graphs[idx] for idx in selected_idx]\n",
    "hiddens, pooleds = model.features(batch_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([624, 7])\n",
      "torch.Size([624, 64])\n",
      "torch.Size([624, 64])\n",
      "torch.Size([624, 64])\n",
      "torch.Size([624, 64])\n"
     ]
    }
   ],
   "source": [
    "for h in hiddens:\n",
    "    print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 624])\n",
      "torch.Size([32, 7])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "for p in pooleds:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = pooleds[2]\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, h = v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 64])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.unsqueeze(2).repeat((1, 1, h)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_feature_distance_matrix = (v.unsqueeze(2).repeat((1, 1, h)) - v.unsqueeze(1)).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000,  43.5648,  46.2721, 161.8035],\n",
       "        [ 43.5648,   0.0000,   2.7072, 118.2386],\n",
       "        [ 46.2721,   2.7072,   0.0000, 115.5314],\n",
       "        [161.8035, 118.2386, 115.5314,   0.0000]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (v.unsqueeze(0).repeat((n, 1, 1)) - v.unsqueeze(1)).sum(dim=2).abs()[:4, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(-43.5648, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(-46.2721, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(-46.2721, device='cuda:0', grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(v[0] - v[0]), torch.sum(v[0] - v[1]), torch.sum(v[0] - v[2]), torch.sum(v[0] - v[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_batch_distance_matrix =  (v.unsqueeze(0).repeat((n, 1, 1)) - v.unsqueeze(1)).sum(dim=2).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_stacked = torch.cat([inter_batch_distance_matrix ** 2, torch.ones((1, n)).cuda()], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_stacked = torch.cat([dist_stacked, torch.ones((n+1, 1)).cuda()], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 6.5267e+02, 8.1258e+03, 1.0000e+00],\n",
       "        [6.5267e+02, 0.0000e+00, 1.3384e+04, 1.0000e+00],\n",
       "        [8.1258e+03, 1.3384e+04, 0.0000e+00, 1.0000e+00],\n",
       "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_stacked[-4:, -4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_stacked[n, n] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 6.5267e+02, 8.1258e+03, 1.0000e+00],\n",
       "        [6.5267e+02, 0.0000e+00, 1.3384e+04, 1.0000e+00],\n",
       "        [8.1258e+03, 1.3384e+04, 0.0000e+00, 1.0000e+00],\n",
       "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_stacked[-4:, -4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = dist_stacked.det()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0., device='cuda:0', grad_fn=<DetBackward>)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplex_volume = (-1 ** (n + 1)) / ((np.math.factorial(n) ** 2) * (2 ** n)) * CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplex_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_batch_distances(M: torch.tensor):\n",
    "    n, h = M.shape\n",
    "    return (M.unsqueeze(0).repeat((n, 1, 1)) - M.unsqueeze(1)).sum(dim=2).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cayley_menger_determinant(D: torch.tensor):\n",
    "    n = D.shape[0]\n",
    "    dist_stacked = torch.cat([D ** 2, torch.ones((1, n)).cuda()], dim=0)\n",
    "    dist_stacked = torch.cat([dist_stacked, torch.ones((n+1, 1)).cuda()], dim=1)\n",
    "    dist_stacked[n, n] = 0.\n",
    "    CM = dist_stacked.det()\n",
    "    return CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplex_volume(n: int, CM: torch.tensor):\n",
    "    return (-1 ** (n + 1)) / ((np.math.factorial(n) ** 2) * (2 ** n)) * CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "0\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(6.1798e-23, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(-3.3556e-20, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(-8.9346e-22, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(7.5531e-22, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "1\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1.0217e-25, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(6.3180e-27, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(-2.6217e-30, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(-4.0057e-35, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n = 16\n",
    "print(n)\n",
    "selected_idx = np.random.permutation(len(train_graphs))[:n]\n",
    "\n",
    "batch_graph = [train_graphs[idx] for idx in selected_idx]\n",
    "labels = torch.LongTensor([graph.label for graph in batch_graph]).to(device)\n",
    "\n",
    "len_a = len((labels == 0).nonzero().squeeze(1).detach().cpu().numpy()) > 0\n",
    "len_b = len((labels == 1).nonzero().squeeze(1).detach().cpu().numpy()) > 0\n",
    "class_a_dict = {'graph': [batch_graph[idx] for idx in (labels == 0).nonzero().squeeze(1)], 'name': '0'} if len_a else None\n",
    "class_b_dict = {'graph': [batch_graph[idx] for idx in (labels == 1).nonzero().squeeze(1)], 'name': '1'} if len_b else None\n",
    "\n",
    "for class_dict in (class_a_dict, class_b_dict):\n",
    "    if class_dict is None:\n",
    "        continue\n",
    "    print(class_dict['name'])\n",
    "    hiddens, pooleds = model.features(class_dict['graph'])\n",
    "    n = len(class_dict['graph'])\n",
    "    for pooled in pooleds[1:]:\n",
    "        print(simplex_volume(n, cayley_menger_determinant(inter_batch_distances(pooled))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(labels == 0).nonzero().squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
