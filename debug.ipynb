{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "from util import load_data, separate_data\n",
    "from models.graphcnn import GraphCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'dataset': 'MUTAG',\n",
    "    'device': 0,\n",
    "    'batch_size': 32,\n",
    "    'iters_per_epoch': 50,\n",
    "    'epochs': 5,\n",
    "    'lr': 0.01,\n",
    "    'seed': 0,\n",
    "    'fold_idx': 0,\n",
    "    'num_layers': 5,\n",
    "    'num_mlp_layers': 2,\n",
    "    'hidden_dim': 3,\n",
    "    'final_dropout': 0.5,\n",
    "    'graph_pooling_type': 'sum',\n",
    "    'neighbor_pooling_type': 'sum',\n",
    "    'learn_eps': False,\n",
    "    'degree_as_tag': False,\n",
    "    'filename': \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_graphs, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_iters = args['iters_per_epoch']\n",
    "    pbar = tqdm(range(total_iters), unit='batch')\n",
    "\n",
    "    loss_accum = 0\n",
    "    for pos in pbar:\n",
    "        selected_idx = np.random.permutation(len(train_graphs))[:args['batch_size']]\n",
    "\n",
    "        batch_graph = [train_graphs[idx] for idx in selected_idx]\n",
    "        output = model(batch_graph)\n",
    "\n",
    "        labels = torch.LongTensor([graph.label for graph in batch_graph]).to(device)\n",
    "\n",
    "        #compute loss\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        #backprop\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()         \n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "        loss = loss.detach().cpu().numpy()\n",
    "        loss_accum += loss\n",
    "\n",
    "        #report\n",
    "        pbar.set_description('epoch: %d' % (epoch))\n",
    "\n",
    "    average_loss = loss_accum/total_iters\n",
    "    print(\"loss training: %f\" % (average_loss))\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "###pass data to model with minibatch during testing to avoid memory overflow (does not perform backpropagation)\n",
    "def pass_data_iteratively(model, graphs, minibatch_size = 64):\n",
    "    model.eval()\n",
    "    output = []\n",
    "    idx = np.arange(len(graphs))\n",
    "    for i in range(0, len(graphs), minibatch_size):\n",
    "        sampled_idx = idx[i:i+minibatch_size]\n",
    "        if len(sampled_idx) == 0:\n",
    "            continue\n",
    "        output.append(model([graphs[j] for j in sampled_idx]).detach())\n",
    "    return torch.cat(output, 0)\n",
    "\n",
    "def test(args, model, device, train_graphs, test_graphs, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    output = pass_data_iteratively(model, train_graphs)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    labels = torch.LongTensor([graph.label for graph in train_graphs]).to(device)\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().cpu().item()\n",
    "    acc_train = correct / float(len(train_graphs))\n",
    "\n",
    "    output = pass_data_iteratively(model, test_graphs)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    labels = torch.LongTensor([graph.label for graph in test_graphs]).to(device)\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().cpu().item()\n",
    "    acc_test = correct / float(len(test_graphs))\n",
    "\n",
    "    print(\"accuracy train: %f test: %f\" % (acc_train, acc_test))\n",
    "\n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatsGraphCNN(GraphCNN):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def __preprocess_neighbors_maxpool(self, batch_graph):\n",
    "        ###create padded_neighbor_list in concatenated graph\n",
    "\n",
    "        #compute the maximum number of neighbors within the graphs in the current minibatch\n",
    "        max_deg = max([graph.max_neighbor for graph in batch_graph])\n",
    "\n",
    "        padded_neighbor_list = []\n",
    "        start_idx = [0]\n",
    "\n",
    "\n",
    "        for i, graph in enumerate(batch_graph):\n",
    "            start_idx.append(start_idx[i] + len(graph.g))\n",
    "            padded_neighbors = []\n",
    "            for j in range(len(graph.neighbors)):\n",
    "                #add off-set values to the neighbor indices\n",
    "                pad = [n + start_idx[i] for n in graph.neighbors[j]]\n",
    "                #padding, dummy data is assumed to be stored in -1\n",
    "                pad.extend([-1]*(max_deg - len(pad)))\n",
    "\n",
    "                #Add center nodes in the maxpooling if learn_eps is False, i.e., aggregate center nodes and neighbor nodes altogether.\n",
    "                if not self.learn_eps:\n",
    "                    pad.append(j + start_idx[i])\n",
    "\n",
    "                padded_neighbors.append(pad)\n",
    "            padded_neighbor_list.extend(padded_neighbors)\n",
    "\n",
    "        return torch.LongTensor(padded_neighbor_list)\n",
    "\n",
    "\n",
    "    def __preprocess_neighbors_sumavepool(self, batch_graph):\n",
    "        ###create block diagonal sparse matrix\n",
    "\n",
    "        edge_mat_list = []\n",
    "        start_idx = [0]\n",
    "        for i, graph in enumerate(batch_graph):\n",
    "            start_idx.append(start_idx[i] + len(graph.g))\n",
    "            edge_mat_list.append(graph.edge_mat + start_idx[i])\n",
    "        Adj_block_idx = torch.cat(edge_mat_list, 1)\n",
    "        Adj_block_elem = torch.ones(Adj_block_idx.shape[1])\n",
    "\n",
    "        #Add self-loops in the adjacency matrix if learn_eps is False, i.e., aggregate center nodes and neighbor nodes altogether.\n",
    "\n",
    "        if not self.learn_eps:\n",
    "            num_node = start_idx[-1]\n",
    "            self_loop_edge = torch.LongTensor([range(num_node), range(num_node)])\n",
    "            elem = torch.ones(num_node)\n",
    "            Adj_block_idx = torch.cat([Adj_block_idx, self_loop_edge], 1)\n",
    "            Adj_block_elem = torch.cat([Adj_block_elem, elem], 0)\n",
    "\n",
    "        Adj_block = torch.sparse.FloatTensor(Adj_block_idx, Adj_block_elem, torch.Size([start_idx[-1],start_idx[-1]]))\n",
    "\n",
    "        return Adj_block.to(self.device)\n",
    "\n",
    "\n",
    "    def __preprocess_graphpool(self, batch_graph):\n",
    "        ###create sum or average pooling sparse matrix over entire nodes in each graph (num graphs x num nodes)\n",
    "        \n",
    "        start_idx = [0]\n",
    "\n",
    "        #compute the padded neighbor list\n",
    "        for i, graph in enumerate(batch_graph):\n",
    "            start_idx.append(start_idx[i] + len(graph.g))\n",
    "\n",
    "        idx = []\n",
    "        elem = []\n",
    "        for i, graph in enumerate(batch_graph):\n",
    "            ###average pooling\n",
    "            if self.graph_pooling_type == \"average\":\n",
    "                elem.extend([1./len(graph.g)]*len(graph.g))\n",
    "            \n",
    "            else:\n",
    "            ###sum pooling\n",
    "                elem.extend([1]*len(graph.g))\n",
    "\n",
    "            idx.extend([[i, j] for j in range(start_idx[i], start_idx[i+1], 1)])\n",
    "        elem = torch.FloatTensor(elem)\n",
    "        idx = torch.LongTensor(idx).transpose(0,1)\n",
    "        graph_pool = torch.sparse.FloatTensor(idx, elem, torch.Size([len(batch_graph), start_idx[-1]]))\n",
    "        \n",
    "        return graph_pool.to(self.device)\n",
    "    \n",
    "    def features(self, batch_graph):\n",
    "        X_concat = torch.cat([graph.node_features for graph in batch_graph], 0).to(self.device)\n",
    "        graph_pool = self.__preprocess_graphpool(batch_graph)\n",
    "\n",
    "        if self.neighbor_pooling_type == \"max\":\n",
    "            padded_neighbor_list = self.__preprocess_neighbors_maxpool(batch_graph)\n",
    "        else:\n",
    "            Adj_block = self.__preprocess_neighbors_sumavepool(batch_graph)\n",
    "\n",
    "        #list of hidden representation at each layer (including input)\n",
    "        hidden_rep = [X_concat]\n",
    "        h = X_concat\n",
    "\n",
    "        for layer in range(self.num_layers-1):\n",
    "            if self.neighbor_pooling_type == \"max\" and self.learn_eps:\n",
    "                h = self.next_layer_eps(h, layer, padded_neighbor_list = padded_neighbor_list)\n",
    "            elif not self.neighbor_pooling_type == \"max\" and self.learn_eps:\n",
    "                h = self.next_layer_eps(h, layer, Adj_block = Adj_block)\n",
    "            elif self.neighbor_pooling_type == \"max\" and not self.learn_eps:\n",
    "                h = self.next_layer(h, layer, padded_neighbor_list = padded_neighbor_list)\n",
    "            elif not self.neighbor_pooling_type == \"max\" and not self.learn_eps:\n",
    "                h = self.next_layer(h, layer, Adj_block = Adj_block)\n",
    "\n",
    "            hidden_rep.append(h)\n",
    "        \n",
    "        pooleds = [graph_pool]\n",
    "        #perform pooling over all nodes in each graph in every layer\n",
    "        for layer, h in enumerate(hidden_rep):\n",
    "            pooled_h = torch.spmm(graph_pool, h)\n",
    "            pooleds.append(pooled_h)\n",
    "            \n",
    "        return hidden_rep, pooleds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "# classes: 2\n",
      "# maximum node tag: 7\n",
      "# data: 188\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#set up seeds and gpu device\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)    \n",
    "device = torch.device(\"cuda:\" + str(args['device'])) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "graphs, num_classes = load_data(args['dataset'], args['degree_as_tag'])\n",
    "\n",
    "##10-fold cross validation. Conduct an experiment on the fold specified by args.fold_idx.\n",
    "train_graphs, test_graphs = separate_data(graphs, args['seed'], args['fold_idx'])\n",
    "\n",
    "model = FeatsGraphCNN(args['num_layers'], args['num_mlp_layers'], train_graphs[0].node_features.shape[1], args['hidden_dim'], num_classes, args['final_dropout'], args['learn_eps'], args['graph_pooling_type'], args['neighbor_pooling_type'], device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgar/anaconda3/envs/geo-const/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "epoch: 1: 100%|██████████| 50/50 [00:01<00:00, 25.85batch/s]\n",
      "epoch: 2:   6%|▌         | 3/50 [00:00<00:01, 26.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 1.775890\n",
      "accuracy train: 0.761905 test: 0.750000\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2: 100%|██████████| 50/50 [00:01<00:00, 25.86batch/s]\n",
      "epoch: 3:   6%|▌         | 3/50 [00:00<00:01, 26.49batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.447053\n",
      "accuracy train: 0.821429 test: 0.800000\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3: 100%|██████████| 50/50 [00:01<00:00, 25.76batch/s]\n",
      "epoch: 4:   6%|▌         | 3/50 [00:00<00:01, 24.86batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.375520\n",
      "accuracy train: 0.839286 test: 0.800000\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4: 100%|██████████| 50/50 [00:01<00:00, 25.51batch/s]\n",
      "epoch: 5:   6%|▌         | 3/50 [00:00<00:01, 25.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.356932\n",
      "accuracy train: 0.857143 test: 0.750000\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5: 100%|██████████| 50/50 [00:01<00:00, 25.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.348208\n",
      "accuracy train: 0.827381 test: 0.800000\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_loss = train(args, model, device, train_graphs, optimizer, epoch)\n",
    "    acc_train, acc_test = test(args, model, device, train_graphs, test_graphs, epoch)\n",
    "\n",
    "    if not args['filename'] == \"\":\n",
    "        with open(args['filename'], 'w') as f:\n",
    "            f.write(\"%f %f %f\" % (avg_loss, acc_train, acc_test))\n",
    "            f.write(\"\\n\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(model.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = np.random.permutation(len(train_graphs))[:args['batch_size']]\n",
    "\n",
    "batch_graph = [train_graphs[idx] for idx in selected_idx]\n",
    "hiddens, pooleds = model.features(batch_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([607, 7])\n",
      "torch.Size([607, 3])\n",
      "torch.Size([607, 3])\n",
      "torch.Size([607, 3])\n",
      "torch.Size([607, 3])\n"
     ]
    }
   ],
   "source": [
    "for h in hiddens:\n",
    "    print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 607])\n",
      "torch.Size([32, 7])\n",
      "torch.Size([32, 3])\n",
      "torch.Size([32, 3])\n",
      "torch.Size([32, 3])\n",
      "torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "for p in pooleds:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = pooleds[2]\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, h = v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.unsqueeze(2).repeat((1, 1, h)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_feature_distance_matrix = (v.unsqueeze(2).repeat((1, 1, h)) - v.unsqueeze(1)).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, 23.7763, 27.4454, 16.6963],\n",
       "        [23.7763,  0.0000,  3.6690,  7.0801],\n",
       "        [27.4454,  3.6690,  0.0000, 10.7491],\n",
       "        [16.6963,  7.0801, 10.7491,  0.0000]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (v.unsqueeze(0).repeat((n, 1, 1)) - v.unsqueeze(1)).sum(dim=2).abs()[:4, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(23.7763, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(27.4454, device='cuda:0', grad_fn=<SumBackward0>),\n",
       " tensor(27.4454, device='cuda:0', grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(v[0] - v[0]), torch.sum(v[0] - v[1]), torch.sum(v[0] - v[2]), torch.sum(v[0] - v[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_batch_distance_matrix =  (v.unsqueeze(0).repeat((n, 1, 1)) - v.unsqueeze(1)).sum(dim=2).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_stacked = torch.cat([inter_batch_distance_matrix ** 2, torch.ones((1, n)).cuda()], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_stacked = torch.cat([dist_stacked, torch.ones((n+1, 1)).cuda()], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000,  93.8940, 417.7198,   1.0000],\n",
       "        [ 93.8940,   0.0000, 115.5260,   1.0000],\n",
       "        [417.7198, 115.5260,   0.0000,   1.0000],\n",
       "        [  1.0000,   1.0000,   1.0000,   1.0000]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_stacked[-4:, -4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_stacked[n, n] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000,  93.8940, 417.7198,   1.0000],\n",
       "        [ 93.8940,   0.0000, 115.5260,   1.0000],\n",
       "        [417.7198, 115.5260,   0.0000,   1.0000],\n",
       "        [  1.0000,   1.0000,   1.0000,   0.0000]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_stacked[-4:, -4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = dist_stacked.det()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<DetBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplex_volume = (-1 ** (n + 1)) / ((np.math.factorial(n) ** 2) * (2 ** n)) * CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0., device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplex_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_feature_distances(M: torch.tensor):\n",
    "    n, h = M.shape\n",
    "    return (M.unsqueeze(2).repeat((1, 1, h)) - M.unsqueeze(1)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_batch_distances(M: torch.tensor):\n",
    "    n, h = M.shape\n",
    "    return (M.unsqueeze(0).repeat((n, 1, 1)) - M.unsqueeze(1)).sum(dim=2) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_batch_distance_vec(M: torch.tensor):\n",
    "    n, h = M.shape\n",
    "    return (M.unsqueeze(0).repeat((n, 1, 1)) - M.unsqueeze(1)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cayley_menger_determinant(D: torch.tensor):\n",
    "    n = D.shape[0]\n",
    "    dist_stacked = torch.cat([D, torch.ones((1, n)).cuda()], dim=0)\n",
    "    dist_stacked = torch.cat([dist_stacked, torch.ones((n+1, 1)).cuda()], dim=1)\n",
    "    dist_stacked[n, n] = 0.\n",
    "    CM = dist_stacked.det()\n",
    "    return CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplex_volume(n: int, CM: torch.tensor):\n",
    "    return (-1 ** (n + 1)) / ((np.math.factorial(n) ** 2) * (2 ** n)) * CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "0\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(-0., device='cuda:0', grad_fn=<DetBackward>)\n",
      "tensor(-0., device='cuda:0', grad_fn=<DetBackward>)\n",
      "tensor(-0., device='cuda:0', grad_fn=<DetBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<DetBackward>)\n",
      "1\n",
      "tensor(-0., device='cuda:0')\n",
      "tensor(-4.5550e-28, device='cuda:0', grad_fn=<DetBackward>)\n",
      "tensor(7.4686e-29, device='cuda:0', grad_fn=<DetBackward>)\n",
      "tensor(0., device='cuda:0', grad_fn=<DetBackward>)\n",
      "tensor(-5.3351e-37, device='cuda:0', grad_fn=<DetBackward>)\n"
     ]
    }
   ],
   "source": [
    "n = 24\n",
    "print(n)\n",
    "selected_idx = np.random.permutation(len(train_graphs))[:n]\n",
    "\n",
    "batch_graph = [train_graphs[idx] for idx in selected_idx]\n",
    "labels = torch.LongTensor([graph.label for graph in batch_graph]).to(device)\n",
    "\n",
    "len_a = len((labels == 0).nonzero().squeeze(1).detach().cpu().numpy()) > 0\n",
    "len_b = len((labels == 1).nonzero().squeeze(1).detach().cpu().numpy()) > 0\n",
    "class_a_dict = {'graph': [batch_graph[idx] for idx in (labels == 0).nonzero().squeeze(1)], 'name': '0'} if len_a else None\n",
    "class_b_dict = {'graph': [batch_graph[idx] for idx in (labels == 1).nonzero().squeeze(1)], 'name': '1'} if len_b else None\n",
    "\n",
    "for class_dict in (class_a_dict, class_b_dict):\n",
    "    if class_dict is None:\n",
    "        continue\n",
    "    print(class_dict['name'])\n",
    "    hiddens, pooleds = model.features(class_dict['graph'])\n",
    "    n = len(class_dict['graph'])\n",
    "    for pooled in pooleds[1:]:\n",
    "#         print(simplex_volume(n, cayley_menger_determinant(inter_batch_distances(pooled))))\n",
    "        print(cayley_menger_determinant(inter_batch_distances(pooled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "17\n",
      "[[20.224258    2.2084515   0.8439397 ]\n",
      " [ 5.9901342   1.2501658   0.78397995]\n",
      " [ 3.3844457   1.1016296   1.0156009 ]\n",
      " [ 1.3948257   1.5147442   0.8645391 ]\n",
      " [ 0.          3.0422933   0.12203759]\n",
      " [ 0.          1.5158514   0.45665187]\n",
      " [14.762299    1.1016296   1.1948199 ]\n",
      " [ 9.257972    2.2032592   1.2259839 ]\n",
      " [ 1.3948257   1.5147442   0.7976161 ]\n",
      " [14.350725    1.1068219   0.63355666]\n",
      " [ 1.4514918   1.5147442   0.8150811 ]\n",
      " [ 0.          1.5161711   0.72425836]\n",
      " [13.294289    1.1016296   1.3076241 ]\n",
      " [ 1.4514918   1.5147442   0.8820039 ]\n",
      " [ 9.258654    2.2026293   1.2262492 ]\n",
      " [ 0.          3.0317028   0.6456123 ]\n",
      " [ 1.3948257   1.5147442   0.9983848 ]]\n",
      "2.355371100388537\n",
      "1\n",
      "7\n",
      "[[0.         1.5158514  0.45665187]\n",
      " [0.         1.5161711  0.6573355 ]\n",
      " [0.         1.5161711  0.38964415]\n",
      " [0.         1.5556002  1.1170869 ]\n",
      " [0.         1.5161711  0.9250269 ]\n",
      " [0.         4.01882    0.51583576]\n",
      " [0.         2.178091   0.40153712]]\n",
      "0.5353837650773908\n"
     ]
    }
   ],
   "source": [
    "for class_dict in (class_a_dict, class_b_dict):\n",
    "    if class_dict is None:\n",
    "        continue\n",
    "    print(class_dict['name'])\n",
    "    hiddens, pooleds = model.features(class_dict['graph'])\n",
    "    n = len(class_dict['graph'])\n",
    "    print(n)\n",
    "    pooled = pooleds[-1].detach().cpu().numpy()\n",
    "    print(pooled)\n",
    "    if class_dict['name'] is 0:\n",
    "        hull = ConvexHull(pooled[:4])\n",
    "        print(hull.volume)\n",
    "    else:\n",
    "        hull = ConvexHull(pooled[:3, 1:])\n",
    "        print(hull.area)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(labels == 0).nonzero().squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "tensor([[[ 0.0000,  0.8859,  1.0149,  ..., 10.9658, 10.4178,  0.8206],\n",
      "         [ 0.8859,  0.0000,  1.9008,  ..., 11.8517, 11.3037,  0.0653],\n",
      "         [ 1.0149,  1.9008,  0.0000,  ...,  9.9509,  9.4029,  1.8356],\n",
      "         ...,\n",
      "         [10.9658, 11.8517,  9.9509,  ...,  0.0000,  0.5480, 11.7865],\n",
      "         [10.4178, 11.3037,  9.4029,  ...,  0.5480,  0.0000, 11.2385],\n",
      "         [ 0.8206,  0.0653,  1.8356,  ..., 11.7865, 11.2385,  0.0000]]],\n",
      "       device='cuda:0', grad_fn=<AbsBackward>)\n",
      "tensor([[[0.0000e+00, 1.4890e+00, 7.9843e-02,  ..., 6.2033e+01,\n",
      "          8.5789e+01, 0.0000e+00],\n",
      "         [1.4890e+00, 0.0000e+00, 1.4092e+00,  ..., 6.0544e+01,\n",
      "          8.4300e+01, 1.4890e+00],\n",
      "         [7.9843e-02, 1.4092e+00, 0.0000e+00,  ..., 6.1953e+01,\n",
      "          8.5709e+01, 7.9843e-02],\n",
      "         ...,\n",
      "         [6.2033e+01, 6.0544e+01, 6.1953e+01,  ..., 0.0000e+00,\n",
      "          2.3756e+01, 6.2033e+01],\n",
      "         [8.5789e+01, 8.4300e+01, 8.5709e+01,  ..., 2.3756e+01,\n",
      "          0.0000e+00, 8.5789e+01],\n",
      "         [0.0000e+00, 1.4890e+00, 7.9843e-02,  ..., 6.2033e+01,\n",
      "          8.5789e+01, 0.0000e+00]]], device='cuda:0', grad_fn=<AbsBackward>)\n",
      "tensor(531.4113, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "tensor(2707.8555, device='cuda:0', grad_fn=<NormBackward0>)\n",
      "tensor([[True],\n",
      "        [True]])\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "print(n)\n",
    "selected_idx = np.random.permutation(len(train_graphs))[:n]\n",
    "selected_idx_different = np.random.permutation(len(train_graphs))[:n]\n",
    "\n",
    "batch_graph = [train_graphs[idx] for idx in selected_idx]\n",
    "batch_graph2 = copy.deepcopy(batch_graph)\n",
    "for i in range(len(batch_graph2)):\n",
    "    batch_graph2[i].node_features[:, 0] += 0.2\n",
    "\n",
    "batch_graph3 = [train_graphs[idx] for idx in selected_idx_different]\n",
    "    \n",
    "print(class_b_dict['name'])\n",
    "hiddens, pooleds = model.features(batch_graph)\n",
    "hiddens2, pooleds2 = model.features(batch_graph2)\n",
    "hiddens3, pooleds3 = model.features(batch_graph3)\n",
    "\n",
    "n = len(class_b_dict['graph'])\n",
    "# for pooled in pooleds[1:]:\n",
    "#         print(simplex_volume(n, cayley_menger_determinant(inter_batch_distances(pooled))))\n",
    "# print(cayley_menger_determinant(inter_batch_distances(pooleds[-1])))\n",
    "# print(cayley_menger_determinant(inter_batch_distances(pooleds[-1])))\n",
    "# print(cayley_menger_determinant(inter_batch_distances(pooleds2[-1])))\n",
    "\n",
    "print(inter_feature_distances(pooleds[-1]))\n",
    "print(inter_feature_distances(pooleds2[-1]))\n",
    "print((pooleds[-1] - pooleds2[-1]).norm())\n",
    "print((pooleds[-1] - pooleds3[-1]).norm())\n",
    "\n",
    "batch_all = np.concatenate([batch_graph, batch_graph2], axis=0)\n",
    "output = pass_data_iteratively(model, batch_all)\n",
    "pred = output.max(1, keepdim=True)[1]\n",
    "labels = torch.LongTensor([graph.label for graph in batch_all]).to(device)\n",
    "correct = pred.eq(labels.view_as(pred)).cpu()\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 929,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph2[0].node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 930,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_graph[0].node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
